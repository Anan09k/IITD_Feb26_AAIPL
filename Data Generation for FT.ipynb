{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7eea64e-82b3-484f-b1aa-cf79227049ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /root/.cache/huggingface /workspace/AAIPL/hf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bd3bbf-f3a5-4a57-bf55-cc93feb0c232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /workspace/AAIPL/hf_models/huggingface/models--Qwen--Qwen2.5-14B-Instruct/snapshots/cf98f3b3bbb457ad9e2bb7baf9a0125b6b88caa8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8886e0dd0ad94ace8112f62d8522e2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on GPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Path where your models exist\n",
    "MODEL_ROOT = \"/workspace/AAIPL/hf_models/huggingface\"\n",
    "\n",
    "MODEL_FOLDER = \"models--Qwen--Qwen2.5-14B-Instruct\"\n",
    "SNAPSHOTS = os.path.join(MODEL_ROOT, MODEL_FOLDER, \"snapshots\")\n",
    "\n",
    "# Get actual snapshot folder\n",
    "snapshot_hash = os.listdir(SNAPSHOTS)[0]\n",
    "LOCAL_MODEL_PATH = os.path.join(SNAPSHOTS, snapshot_hash)\n",
    "\n",
    "print(\"Loading model from:\", LOCAL_MODEL_PATH)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    local_files_only=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully on GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "170b15f7-0439-4f77-b240-1386969bb092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(topic: str) -> str:\n",
    "    import random\n",
    "    SYLLOGISM_DOMAINS = [\n",
    "    \"Professions\",\n",
    "    \"Scientific fields\",\n",
    "    \"Companies and organizations\",\n",
    "    \"Geographic locations\",\n",
    "    \"Vehicles and machines\",\n",
    "    \"Academic disciplines\",\n",
    "    \"Musical genres\",\n",
    "    \"Technologies\",\n",
    "    \"Historical groups\",\n",
    "    \"Kitchen utensils\",\n",
    "    \"Sports teams\"\n",
    "]\n",
    "\n",
    "    topic_constraints = \"\"\n",
    "    if topic == \"Syllogisms\":\n",
    "        domain = random.choice(SYLLOGISM_DOMAINS)\n",
    "        topic_constraints = \"\"\"\n",
    "- Minimum 3 premises.\n",
    "- Include at least one universal (All/No) and one particular (Some/Not all).\n",
    "- Include at least one negative or restrictive statement.\n",
    "- Require interaction of at least TWO premises.\n",
    "- Avoid trivial direct chaining (Aâ†’Bâ†’C only).\n",
    "- Avoid repetition of a single domain.\n",
    "- Use categories related to: {domain}.\n",
    "- At least one distractor must appear logically plausible.\n",
    "\"\"\"\n",
    "\n",
    "    elif topic == \"Seating Arrangements (Circular and Linear)\":\n",
    "        topic_constraints = \"\"\"\n",
    "- 6 to 8 named individuals.\n",
    "- Clearly state whether arrangement is Circular or Linear.\n",
    "- At least 4 constraints.\n",
    "- Include at least one negative constraint (e.g., not adjacent).\n",
    "- Require multi-step deduction.\n",
    "\"\"\"\n",
    "\n",
    "    elif topic == \"Blood Relations and Family Tree\":\n",
    "        topic_constraints = \"\"\"\n",
    "- Minimum 5 individuals.\n",
    "- Include at least TWO generations.\n",
    "- Require at least TWO inferential steps.\n",
    "- Avoid trivial sibling-only relations.\n",
    "- Include indirect relations (e.g., maternal uncle, niece, grandfather).\n",
    "- The final question must not be answerable in one obvious step.\n",
    "\"\"\"\n",
    "\n",
    "    elif topic == \"Mixed Series (Alphanumeric)\":\n",
    "        topic_constraints = \"\"\"\n",
    "- Combine letters and numbers.\n",
    "- Pattern must require at least TWO operations.\n",
    "- Avoid simple +1 increments.\n",
    "- Require reasoning to detect hidden pattern.\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are generating ONE difficult logical reasoning MCQ.\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Requirements:\n",
    "{topic_constraints}\n",
    "\n",
    "General Rules:\n",
    "- Exactly 4 answer choices labeled A, B, C, D.\n",
    "- Exactly ONE correct answer.\n",
    "- No duplicate choices.\n",
    "- Explanation must be concise (maximum 80 words).\n",
    "- Output ONLY valid JSON.\n",
    "- Do NOT include markdown.\n",
    "- Do NOT include text outside JSON.\n",
    "- Generate completely NEW content.\n",
    "\n",
    "Return JSON in this exact structure:\n",
    "\n",
    "{{\n",
    "    \"topic\": \"{topic}\",\n",
    "    \"question\": \"Your question ending with a question mark?\",\n",
    "    \"choices\": [\n",
    "        \"A) First option\",\n",
    "        \"B) Second option\",\n",
    "        \"C) Third option\",\n",
    "        \"D) Fourth option\"\n",
    "    ],\n",
    "    \"answer\": \"A\",\n",
    "    \"explanation\": \"Concise reasoning explanation.\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffbb0ec2-35a9-4380-b46e-0a3962fc0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_question(topic):\n",
    "\n",
    "    prompt = build_prompt(topic)\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    chat = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(chat, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    new_tokens = output_ids[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Extract JSON safely\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback: extract JSON block if extra whitespace\n",
    "        match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(0))\n",
    "            except:\n",
    "                return None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46cbc11a-02a0-446f-97e7-c975a79d5665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Blood Relations',\n",
       " 'question': \"If X is the brother of Y's father's only daughter and Z is the son of X, how is Z related to Y?\",\n",
       " 'choices': ['A) Nephew', 'B) Brother', 'C) Cousin', 'D) Uncle'],\n",
       " 'answer': 'C',\n",
       " 'explanation': \"X is Y's aunt, making Z Y's cousin.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = generate_question(\"Blood Relations\")\n",
    "q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "486008d3-1f1e-497e-b420-0f99cb6981b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Syllogisms',\n",
       " 'question': 'Some artists are not painters. All sculptors are painters. Not all artists are sculptors. Which of the following can be concluded?',\n",
       " 'choices': ['A) Some artists are not sculptors.',\n",
       "  'B) All painters are artists.',\n",
       "  'C) No sculptor is an artist.',\n",
       "  'D) Some sculptors are not artists.'],\n",
       " 'answer': 'A',\n",
       " 'explanation': \"From 'Some artists are not painters' and 'All sculptors are painters', it follows that these non-painter artists cannot be sculptors, thus some artists are not sculptors.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = generate_question(\"Syllogisms\")\n",
    "q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e50eb5ef-4ed0-49f0-865d-01a1f3a7ae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 950\n",
      "New length: 890\n",
      "Last 60 entries removed successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "FILE_PATH = \"generated_mcq_dataset.json\"\n",
    "\n",
    "with open(FILE_PATH, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "original_length = len(data)\n",
    "print(\"Original length:\", original_length)\n",
    "\n",
    "# Remove last 60 entries safely\n",
    "if original_length >= 60:\n",
    "    data = data[:-60]\n",
    "else:\n",
    "    print(\"Warning: Less than 60 entries found.\")\n",
    "    data = []\n",
    "\n",
    "print(\"New length:\", len(data))\n",
    "\n",
    "# Save back to file\n",
    "with open(FILE_PATH, \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"Last 60 entries removed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4644a97-0df8-4fb6-8ad6-3d175bbe11f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating for topic: Syllogisms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [8:22:22<00:00, 120.57s/it]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating for topic: Seating Arrangements (Circular and Linear)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating for topic: Blood Relations and Family Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating for topic: Mixed Series (Alphanumeric)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset generation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "stop_generation =False\n",
    "OUTPUT_FILE = \"generated_mcq_dataset (1).json\"\n",
    "TARGET_PER_TOPIC = 300\n",
    "PAUSE_INTERVAL = 50\n",
    "\n",
    "TOPICS = [\n",
    "    \"Syllogisms\",\n",
    "    \"Seating Arrangements (Circular and Linear)\",\n",
    "    \"Blood Relations and Family Tree\",\n",
    "    \"Mixed Series (Alphanumeric)\"\n",
    "]\n",
    "\n",
    "def validate_mcq(data):\n",
    "    if not isinstance(data, dict):\n",
    "        return False\n",
    "    required = {\"topic\", \"question\", \"choices\", \"answer\", \"explanation\"}\n",
    "    if not required.issubset(data.keys()):\n",
    "        return False\n",
    "    if len(data[\"choices\"]) != 4:\n",
    "        return False\n",
    "    if data[\"answer\"] not in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Load existing dataset (resume safe)\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    with open(OUTPUT_FILE, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "else:\n",
    "    dataset = []\n",
    "\n",
    "existing_questions = set(item[\"question\"] for item in dataset)\n",
    "\n",
    "for topic in TOPICS:\n",
    "    print(f\"\\nGenerating for topic: {topic}\")\n",
    "    topic_count = sum(1 for item in dataset if item[\"topic\"] == topic)\n",
    "\n",
    "    pbar = tqdm(total=TARGET_PER_TOPIC - topic_count)\n",
    "\n",
    "    while topic_count < TARGET_PER_TOPIC:\n",
    "        if stop_generation:\n",
    "            break\n",
    "            \n",
    "        data = generate_question(topic)\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        if not validate_mcq(data):\n",
    "            continue\n",
    "\n",
    "        if data[\"question\"] in existing_questions:\n",
    "            continue\n",
    "\n",
    "        dataset.append(data)\n",
    "        existing_questions.add(data[\"question\"])\n",
    "        topic_count += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Save every 10\n",
    "        if len(dataset) % 10 == 0:\n",
    "            with open(OUTPUT_FILE, \"w\") as f:\n",
    "                json.dump(dataset, f, indent=2)\n",
    "\n",
    "        # ðŸ”¥ Pause every 50 per topic\n",
    "        if topic_count % PAUSE_INTERVAL == 0:\n",
    "            # Save before pause\n",
    "            with open(OUTPUT_FILE, \"w\") as f:\n",
    "                json.dump(dataset, f, indent=2)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"PAUSED at {topic_count} questions for topic: {topic}\")\n",
    "            print(\"Preview of last 3 questions:\\n\")\n",
    "\n",
    "            for item in dataset[-3:]:\n",
    "                print(f\"Q: {item['question']}\")\n",
    "                print(f\"Answer: {item['answer']}\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "            user_input = input(\"Type 'c' to continue or 'q' to stop safely: \")\n",
    "\n",
    "            if user_input.lower() == 'q':\n",
    "                print(\"Stopping safely. Progress saved.\")\n",
    "                stop_generation = True\n",
    "                break\n",
    "\n",
    "    pbar.close()\n",
    "    if stop_generation:\n",
    "        break\n",
    "\n",
    "# Final Save\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)\n",
    "\n",
    "print(\"\\nDataset generation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b03ee-4a61-461b-8962-9f8a44b5c3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa84bd6-8375-40c3-b87a-df619c64996d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
