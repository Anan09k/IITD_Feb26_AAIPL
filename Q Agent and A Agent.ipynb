{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18d3890a-ac13-433c-9331-02abfa3ce33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x git.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1857b040-bcc1-4c7c-a17e-24adb99f78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/AAIPL/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/workspace/AAIPL/hf_cache/datasets\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/workspace/AAIPL/hf_cache/models\"\n",
    "\n",
    "os.makedirs(\"/workspace/AAIPL/hf_cache/datasets\", exist_ok=True)\n",
    "os.makedirs(\"/workspace/AAIPL/hf_cache/models\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04af1e4c-6cab-415d-a3d8-240ca58a9e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1200\n",
      "Valid: 1200\n",
      "Invalid: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "FILE = \"generated_mcq_dataset (1).json\"\n",
    "\n",
    "with open(FILE, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def validate_schema(item):\n",
    "    if not isinstance(item, dict):\n",
    "        return False\n",
    "    \n",
    "    required = {\"topic\", \"question\", \"choices\", \"answer\", \"explanation\"}\n",
    "    if not required.issubset(item.keys()):\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(item[\"choices\"], list) or len(item[\"choices\"]) != 4:\n",
    "        return False\n",
    "    \n",
    "    if item[\"answer\"] not in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(item[\"question\"], str) or len(item[\"question\"]) < 20:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "valid = [item for item in data if validate_schema(item)]\n",
    "\n",
    "print(\"Total:\", len(data))\n",
    "print(\"Valid:\", len(valid))\n",
    "print(\"Invalid:\", len(data) - len(valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d15bfaa-f591-47f5-9be6-4c4eb3f7943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded samples: 1200\n",
      "Balanced samples: 1200\n",
      "Saved to: a_agent_sft_balanced.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "INPUT_FILE = \"generated_mcq_dataset (1).json\"\n",
    "OUTPUT_FILE = \"a_agent_sft_balanced.jsonl\"\n",
    "\n",
    "with open(INPUT_FILE, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(\"Loaded samples:\", len(dataset))\n",
    "\n",
    "balanced_data = []\n",
    "\n",
    "for example in dataset:\n",
    "\n",
    "    question = example[\"question\"]\n",
    "    choices = example[\"choices\"]  # list of strings like \"A) text\"\n",
    "    correct_letter = example[\"answer\"]\n",
    "    explanation = example[\"explanation\"]\n",
    "\n",
    "    # Extract letter + text\n",
    "    parsed_choices = []\n",
    "    for choice in choices:\n",
    "        match = re.match(r\"([ABCD])\\)\\s*(.*)\", choice)\n",
    "        if not match:\n",
    "            continue\n",
    "        parsed_choices.append(match.groups())\n",
    "\n",
    "    if len(parsed_choices) != 4:\n",
    "        continue\n",
    "\n",
    "    option_dict = {letter: text for letter, text in parsed_choices}\n",
    "\n",
    "    # Shuffle letters\n",
    "    letters = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    new_letters = letters.copy()\n",
    "    random.shuffle(new_letters)\n",
    "\n",
    "    mapping = dict(zip(letters, new_letters))\n",
    "\n",
    "    # Rebuild shuffled choices\n",
    "    new_choices = []\n",
    "    for old_letter in letters:\n",
    "        new_letter = mapping[old_letter]\n",
    "        new_choices.append(f\"{new_letter}) {option_dict[old_letter]}\")\n",
    "\n",
    "    # Update correct answer\n",
    "    new_correct_letter = mapping[correct_letter]\n",
    "\n",
    "    # Build chat-format structure for SFT\n",
    "    balanced_data.append({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert logical reasoning assistant. Carefully analyze the MCQ and respond with the correct answer and a concise explanation in JSON format.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question + \"\\n\\n\" + \"\\n\".join(new_choices)\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": json.dumps({\n",
    "                    \"answer\": new_correct_letter,\n",
    "                    \"reasoning\": explanation\n",
    "                })\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "print(\"Balanced samples:\", len(balanced_data))\n",
    "\n",
    "# Save as JSONL\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    for item in balanced_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(\"Saved to:\", OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf57f27d-d558-423e-bd59-b304c6619f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'C': 311, 'D': 304, 'B': 301, 'A': 284})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "counts = Counter()\n",
    "\n",
    "with open(\"a_agent_sft_balanced.jsonl\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        ans = json.loads(obj[\"messages\"][2][\"content\"])[\"answer\"]\n",
    "        counts[ans] += 1\n",
    "\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ca59c57-7b67-4617-8f23-04d518663a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /workspace/AAIPL/hf_models/huggingface/models--Unsloth--Llama-3.1-8B-Instruct/snapshots/4699cc75b550f9c6f3173fb80f4703b62d946aa5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ad6ab0500d41f5a0334960cae81143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fd4b38061e4915a0a428413fd91363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# -------- LOCAL MODEL PATH --------\n",
    "\n",
    "MODEL_ROOT = \"/workspace/AAIPL/hf_models/huggingface\"\n",
    "MODEL_FOLDER = \"models--Unsloth--Llama-3.1-8B-Instruct\"\n",
    "SNAPSHOTS = os.path.join(MODEL_ROOT, MODEL_FOLDER, \"snapshots\")\n",
    "\n",
    "snapshot_hash = os.listdir(SNAPSHOTS)[0]\n",
    "LOCAL_MODEL_PATH = os.path.join(SNAPSHOTS, snapshot_hash)\n",
    "\n",
    "print(\"Loading model from:\", LOCAL_MODEL_PATH)\n",
    "\n",
    "DATA_PATH = \"/workspace/AAIPL/a_agent_sft_balanced.jsonl\"\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=DATA_PATH,\n",
    "    cache_dir=\"/workspace/AAIPL/hf_cache/datasets\"\n",
    ")[\"train\"]\n",
    "\n",
    "split_dataset = dataset.train_test_split(\n",
    "    test_size=0.1,  # 10% validation\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    local_files_only=True,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21fadc19-a83b-4cc5-a551-ed799bf371be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "666ca2d3-85dd-4467-83b2-d6ffb82bdda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA model ready.\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(\"LoRA model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3efbe720-376c-42b8-bdaa-a6f86fc4b052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21aec9215a104e3d985687fe1c46cd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dabbb645e7d429bb0e79acafaa56398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1fa903f4a940a292db9c4f2cfbbacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b83cba72c647b5ae2237a00cccef24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='204' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [204/204 01:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.360281</td>\n",
       "      <td>137331.000000</td>\n",
       "      <td>0.874073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>0.315344</td>\n",
       "      <td>0.302131</td>\n",
       "      <td>273061.000000</td>\n",
       "      <td>0.883108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.306232</td>\n",
       "      <td>0.259197</td>\n",
       "      <td>408405.000000</td>\n",
       "      <td>0.887783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.259400</td>\n",
       "      <td>0.302248</td>\n",
       "      <td>0.263603</td>\n",
       "      <td>545384.000000</td>\n",
       "      <td>0.889302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/workspace/AAIPL/a_agent_lora\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    do_eval=True,\n",
    "    eval_strategy = \"steps\",\n",
    "    eval_steps = 50,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset =val_dataset,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"/workspace/AAIPL/a_agent_lora_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b67b218-753f-4c38-84b1-bc68d2abb992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3489009737968445, 'eval_runtime': 1.1542, 'eval_samples_per_second': 103.971, 'eval_steps_per_second': 12.996, 'eval_entropy': 0.3602811018625895, 'eval_num_tokens': 137331.0, 'eval_mean_token_accuracy': 0.87407306432724, 'epoch': 0.7407407407407407, 'step': 50}\n",
      "{'eval_loss': 0.3153443932533264, 'eval_runtime': 1.1485, 'eval_samples_per_second': 104.489, 'eval_steps_per_second': 13.061, 'eval_entropy': 0.30213125546773273, 'eval_num_tokens': 273061.0, 'eval_mean_token_accuracy': 0.8831080238024394, 'epoch': 1.474074074074074, 'step': 100}\n",
      "{'eval_loss': 0.3062315285205841, 'eval_runtime': 1.1506, 'eval_samples_per_second': 104.295, 'eval_steps_per_second': 13.037, 'eval_entropy': 0.25919678807258606, 'eval_num_tokens': 408405.0, 'eval_mean_token_accuracy': 0.887783149878184, 'epoch': 2.2074074074074073, 'step': 150}\n",
      "{'eval_loss': 0.3022478222846985, 'eval_runtime': 1.1489, 'eval_samples_per_second': 104.446, 'eval_steps_per_second': 13.056, 'eval_entropy': 0.2636028826236725, 'eval_num_tokens': 545384.0, 'eval_mean_token_accuracy': 0.8893021623293559, 'epoch': 2.948148148148148, 'step': 200}\n"
     ]
    }
   ],
   "source": [
    "for log in trainer.state.log_history:\n",
    "    if \"eval_loss\" in log:\n",
    "        print(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "942a93a7-c147-42a8-a778-1cf77d7c673d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec304fa5bbbc46ecbaa471d96b2cc424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"/workspace/AAIPL/a_agent_lora_final\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b95b2d6b-3959-410f-ac59-cd3f13c27dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "print(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47ecac71-28e0-4dae-bf66-fdc2610b1785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|██████████| 15/15 [00:22<00:00,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6083333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "\n",
    "batch_size = 8\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in tqdm(range(0, len(val_dataset), batch_size)):\n",
    "\n",
    "    batch = val_dataset.select(\n",
    "        range(i, min(i+batch_size, len(val_dataset)))\n",
    "    )\n",
    "\n",
    "    prompts = []\n",
    "    true_answers = []\n",
    "\n",
    "    for example in batch:\n",
    "\n",
    "        messages = example[\"messages\"]\n",
    "        system_msg = messages[0]\n",
    "        user_msg = messages[1]\n",
    "        assistant_msg = messages[2]\n",
    "\n",
    "        true_answer = json.loads(\n",
    "            assistant_msg[\"content\"]\n",
    "        )[\"answer\"]\n",
    "\n",
    "        true_answers.append(true_answer)\n",
    "\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            [system_msg, user_msg],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=60,        # enough for JSON\n",
    "            do_sample=False,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    for text, true_ans in zip(decoded, true_answers):\n",
    "\n",
    "        # Extract only generated part\n",
    "        # (everything after prompt)\n",
    "        try:\n",
    "            json_start = text.find(\"{\")\n",
    "            json_str = text[json_start:]\n",
    "            pred_json = json.loads(json_str)\n",
    "\n",
    "            predicted_answer = pred_json[\"answer\"]\n",
    "\n",
    "            if predicted_answer == true_ans:\n",
    "                correct += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b4e900e-16a8-43ec-8a6d-0739a3b8c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total topics found: 4\n",
      "Created 4 questions in outputs/filtered_questions.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "INPUT_FILE = \"generated_mcq_dataset (1).json\"\n",
    "OUTPUT_FILE = \"outputs/filtered_questions.json\"\n",
    "\n",
    "# Load original dataset\n",
    "with open(INPUT_FILE, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Group by topic\n",
    "topic_map = {}\n",
    "\n",
    "for item in dataset:\n",
    "    topic = item[\"topic\"]\n",
    "    if topic not in topic_map:\n",
    "        topic_map[topic] = item  # take first occurrence\n",
    "\n",
    "print(\"Total topics found:\", len(topic_map))\n",
    "\n",
    "# Convert to answer_agent expected format\n",
    "filtered_questions = []\n",
    "\n",
    "for topic, item in topic_map.items():\n",
    "    filtered_questions.append({\n",
    "        \"topic\": topic,\n",
    "        \"question\": item[\"question\"],\n",
    "        \"choices\": item[\"choices\"]  # already formatted as A) ...\n",
    "    })\n",
    "\n",
    "# Ensure outputs folder exists\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Save\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(filtered_questions, f, indent=4)\n",
    "\n",
    "print(f\"Created {len(filtered_questions)} questions in {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afc5cf3f-9184-4ff0-99ef-083d04381e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:08<00:00,  2.14s/it]\n",
      "STEPS:   0%|                                           | 0/1 [00:00<?, ?batch/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "STEPS: : 2batch [00:18,  9.08s/batch]                                           \n",
      "\n",
      "=== Question 1 ===\n",
      "Question: If all cats are animals that can climb trees, and some animals that can climb trees are not dogs, and no dogs are cats, which of the following must be true?\n",
      "Expected: N/A\n",
      "Model Answer:\n",
      "{\"answer\": \"C\", \"reasoning\": \"Since all cats can climb trees but no dogs are cats, it follows logically that no animals that can climb trees could also be dogs.\"}\n",
      "\n",
      "=== Question 2 ===\n",
      "Question: In a circular arrangement of six friends - Alice, Bob, Carol, Dave, Eve, and Frank - Alice is not next to Bob. Carol sits exactly between Dave and Eve. Frank is seated directly opposite to Dave. Who can sit next to Bob? \n",
      "Expected: N/A\n",
      "Model Answer:\n",
      "{\"answer\": \"C\", \"reasoning\": \"Since Carol is between Dave and Eve, she cannot sit next to Bob due to her fixed position relative to others. Given Frank's opposition to Dave, he also cannot sit beside Bob. Thus, either Alice or Eve could theoretically fit but since Alice isn't adjacent to Bob, it leaves Eve as the viable option.\"}\n",
      "\n",
      "=== Question 3 ===\n",
      "Question: If X is the maternal uncle of Y, and Z is the father of X, who is Z to W if W is the sister of Y?\n",
      "Expected: N/A\n",
      "Model Answer:\n",
      "{\"answer\": \"C\", \"reasoning\": \"X is Y's maternal uncle; thus, he shares his mother with Y but not necessarily any other relatives directly unless specified otherwise. Since Z is X's father, making him also Y's grandfather through X.\"}\n",
      "\n",
      "=== Question 4 ===\n",
      "Question: What is the next term in the series? B2, D5, G9, K14, ?\n",
      "Expected: N/A\n",
      "Model Answer:\n",
      "{\"answer\": \"B\", \"reasoning\": \"The letters follow an increment pattern where each subsequent letter skips one position forward while adding two to its value except for every third term which adds three instead of two.\"}\n",
      "BATCH - 0\n",
      "Tokens: 304, Time: 16.186 seconds\n",
      "TGPS: 18.782 seconds\n",
      "BATCH - 1\n",
      "Tokens: 304, Time: 1.968 seconds\n",
      "TGPS: 154.470 seconds\n",
      "\n",
      "==================================================\n",
      "Total Time: 18.154 seconds; Total Tokens: 608; TGPS: 33.491 seconds\n"
     ]
    }
   ],
   "source": [
    "!python -m agents.answer_agent \\\n",
    "  --input_file outputs/filtered_questions.json \\\n",
    "  --output_file outputs/answers.json \\\n",
    "  --verbose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344c724-43e3-4fd9-90fc-477ab5a0e155",
   "metadata": {},
   "source": [
    "## Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83b9e9b-edaf-4559-b257-5f3ae989f4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /workspace/AAIPL/hf_models/huggingface/models--Unsloth--Llama-3.1-8B-Instruct/snapshots/4699cc75b550f9c6f3173fb80f4703b62d946aa5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a059b72a61f4635ac2b700e4c5514a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "MODEL_ROOT = \"/workspace/AAIPL/hf_models/huggingface\"\n",
    "MODEL_FOLDER = \"models--Unsloth--Llama-3.1-8B-Instruct\"\n",
    "SNAPSHOTS = os.path.join(MODEL_ROOT, MODEL_FOLDER, \"snapshots\")\n",
    "\n",
    "snapshot_hash = os.listdir(SNAPSHOTS)[0]\n",
    "LOCAL_MODEL_PATH = os.path.join(SNAPSHOTS, snapshot_hash)\n",
    "\n",
    "print(\"Loading model from:\", LOCAL_MODEL_PATH)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    local_files_only=True,\n",
    "    use_fast=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8978ffd4-2b14-43d1-a4b2-4ca685913333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(topic: str) -> str:\n",
    "\n",
    "    import random\n",
    "\n",
    "    SYLLOGISM_DOMAINS = [\n",
    "        \"Professions\",\n",
    "        \"Scientific fields\",\n",
    "        \"Companies and organizations\",\n",
    "        \"Geographic locations\",\n",
    "        \"Vehicles and machines\",\n",
    "        \"Academic disciplines\",\n",
    "        \"Musical genres\",\n",
    "        \"Technologies\",\n",
    "        \"Historical groups\",\n",
    "        \"Kitchen utensils\",\n",
    "        \"Sports teams\"\n",
    "    ]\n",
    "\n",
    "    topic_constraints = \"\"\n",
    "\n",
    "    if topic == \"Syllogisms\":\n",
    "        domain = random.choice(SYLLOGISM_DOMAINS)\n",
    "    \n",
    "        topic_constraints = f\"\"\"\n",
    "    STRICT LOGIC FORMAT (MANDATORY):\n",
    "    \n",
    "    Each premise must follow EXACTLY one of these forms:\n",
    "    \n",
    "    - All A are B.\n",
    "    - No A are B.\n",
    "    - Some A are B.\n",
    "    - Some A are not B.\n",
    "    \n",
    "    Replace A and B with categorical nouns from this domain: {domain}.\n",
    "    Use short abstract category names (e.g., \"Architects\", \"Engineers\", \"Universities\").\n",
    "    \n",
    "    Do NOT use:\n",
    "    - Explanatory phrases\n",
    "    - Relative clauses (that, which, who)\n",
    "    - Property-based statements (e.g., \"companies that own banks\")\n",
    "    - Real-world factual claims\n",
    "    \n",
    "    Write EXACTLY FOUR premises labeled:\n",
    "    \n",
    "    Premise 1:\n",
    "    Premise 2:\n",
    "    Premise 3:\n",
    "    Premise 4:\n",
    "    \n",
    "    The final question must be:\n",
    "    \"Which of the following conclusions logically follows?\"\n",
    "    \n",
    "    All answer choices must also strictly follow the same categorical structure.\n",
    "    \n",
    "    The correct conclusion must be logically deducible by formal syllogistic reasoning.\n",
    "    Distractors must include:\n",
    "    - One with reversed quantifier\n",
    "    - One with undistributed middle error\n",
    "    - One invalid existential leap\n",
    "    \n",
    "    Do NOT invent factual reasoning.\n",
    "    Only use formal logical deduction.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    elif topic == \"Seating Arrangements (Circular and Linear)\":\n",
    "\n",
    "        topic_constraints = \"\"\"\n",
    "- Use EXACTLY 7 individuals with distinct names.\n",
    "- Clearly state whether arrangement is Circular or Linear at the beginning.\n",
    "- Include at least:\n",
    "    • Two adjacency constraints,\n",
    "    • One directional left/right constraint,\n",
    "    • One negative constraint (not adjacent / not opposite),\n",
    "    • One positional constraint relative to two people.\n",
    "- At least THREE constraints must interact to determine the answer.\n",
    "- Ensure the solution is logically unique.\n",
    "- Avoid simple linear chaining placements.\n",
    "\"\"\"\n",
    "\n",
    "    elif topic == \"Blood Relations and Family Tree\":\n",
    "\n",
    "        topic_constraints = \"\"\"\n",
    "- Include EXACTLY 6 individuals across at least THREE generations.\n",
    "- Include at least one marriage relation.\n",
    "- Use indirect relations such as maternal uncle, paternal aunt, niece, grandson.\n",
    "- The final question must require tracing through at least TWO inferential steps.\n",
    "- Avoid trivial parent-child-only deductions.\n",
    "- The answer must NOT be derivable from a single sentence alone.\n",
    "\"\"\"\n",
    "\n",
    "    elif topic == \"Mixed Series (Alphanumeric)\":\n",
    "\n",
    "        topic_constraints = \"\"\"\n",
    "- Sequence must contain at least 5 visible terms with one missing term.\n",
    "- Must combine BOTH:\n",
    "    • A non-linear numerical rule (multiplication, division, squares, alternating operations, or layered pattern).\n",
    "    • A non-linear letter progression (variable jumps, cyclic shifts, alternating increments).\n",
    "- DO NOT use simple +1 or constant-addition patterns.\n",
    "- The pattern must require identifying at least TWO interacting rules.\n",
    "- Letters and numbers must NOT change independently in a trivial manner.\n",
    "- The answer must require analyzing BOTH components together.\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are generating ONE difficult logical reasoning MCQ.\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Requirements:\n",
    "{topic_constraints}\n",
    "\n",
    "General Rules:\n",
    "- Exactly 4 answer choices labeled A, B, C, D.\n",
    "- Exactly ONE correct answer.\n",
    "- No duplicate choices.\n",
    "- The distractors must be logically plausible.\n",
    "- Difficulty must be higher than typical competitive exam questions.\n",
    "- Explanation must be concise (maximum 80 words).\n",
    "- Output ONLY valid JSON.\n",
    "- Do NOT include markdown.\n",
    "- Do NOT include text outside JSON.\n",
    "- Generate completely NEW content.\n",
    "- Internally verify the answer before outputting.\n",
    "\n",
    "Return JSON in this exact structure:\n",
    "\n",
    "{{\n",
    "    \"topic\": \"{topic}\",\n",
    "    \"question\": \"Your question ending with a question mark?\",\n",
    "    \"choices\": [\n",
    "        \"A) First option\",\n",
    "        \"B) Second option\",\n",
    "        \"C) Third option\",\n",
    "        \"D) Fourth option\"\n",
    "    ],\n",
    "    \"answer\": \"A\",\n",
    "    \"explanation\": \"Concise reasoning explanation.\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a4c6d1-d262-4150-904a-df7e78a1b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_clean(topic):\n",
    "    prompt = build_prompt(topic)\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    chat = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(chat, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty = 1.2,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    new_tokens = output_ids[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Extract only JSON block\n",
    "    import re, json\n",
    "    match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34997925-9555-4eb0-b9bd-2d7129adac3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"topic\": \"Syllogisms\",\\n    \"question\": \"All Geologists are Researchers. Some Researchers are Professors. Some Professors are Scientists. Which of the following conclusions logically follows?\",\\n    \"choices\": [\\n        \"A) Some Scientists are Geologists.\",\\n        \"B) All Scientists are Researches.\",\\n        \"C) Some Researchers are Not Scientists.\",\\n        \"D) All Geologists are Scientists.\"\\n    ],\\n    \"answer\": \"A\",\\n    \"explanation\": \"From \\'Some Researchers are Professors\\' we get \\'Some Professors are Researchers\\'. Then using transitivity with \\'All Geologists are Researchers\\', it\\'s inferred as some geologist is researcher; then applying\\'some researchers are professors\\', therefore some geologists are professors; since all scientists are not explicitly stated to have any subset or superset relationship so its safe to conclude\\'some scientists are professors\\'; Now given this result along with last statement some professoer are scientist, hence inferencing\\'some professor are scienctist\\' means our previous inference of\\'some scientists being professior was wrong if they were exclusively profesoor so combining both results now we can safely say that some scientisit should be equalt to geoligists because there cant be two different groups where first group consist only Profesoor but second group contain atleast those profesoosr whome are not necesarily profesooor, Hence Conclusiion = A)\"\\n}'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = generate_question_clean(\"Syllogisms\")\n",
    "q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8ea773d-575a-4913-a4f8-725c140ed5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# IMPORTANT: use processing_class instead of tokenizer\u001b[39;00m\n\u001b[32m     38\u001b[39m trainer = SFTTrainer(\n\u001b[32m     39\u001b[39m     model=model,\n\u001b[32m     40\u001b[39m     processing_class=tokenizer,\n\u001b[32m     41\u001b[39m     train_dataset=dataset,\n\u001b[32m     42\u001b[39m     args=training_args,\n\u001b[32m     43\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33m./q_agent_sft_final\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:1189\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1188\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py:4009\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4008\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4009\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4011\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4012\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4013\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4014\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4015\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:1103\u001b[39m, in \u001b[36mSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;66;03m# If not set, defaults from model config and may warn since cache isn't compatible with gradient checkpointing\u001b[39;00m\n\u001b[32m   1102\u001b[39m inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m (loss, outputs) = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[38;5;66;03m# Add auxiliary loss if available\u001b[39;00m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aux_loss_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aux_loss_coef:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py:4099\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4097\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4098\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4099\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4100\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4101\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py:819\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py:807\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py:1850\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1849\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1850\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1861\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1863\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:222\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py:1850\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1849\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1850\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1861\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1863\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:222\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1062\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py:395\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    407\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    408\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    409\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py:294\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py:236\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    233\u001b[39m input_shape = hidden_states.shape[:-\u001b[32m1\u001b[39m]\n\u001b[32m    234\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m query_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    237\u001b[39m key_states = \u001b[38;5;28mself\u001b[39m.k_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    238\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:757\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    755\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.base_layer(x, *args, **kwargs)\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    758\u001b[39m     torch_result_dtype = result.dtype\n\u001b[32m    760\u001b[39m     lora_A_keys = \u001b[38;5;28mself\u001b[39m.lora_A.keys()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"a_agent_sft_balanced.jsonl\"\n",
    ")[\"train\"]\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./q_agent_sft\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# IMPORTANT: use processing_class instead of tokenizer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./q_agent_sft_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42015a18-cc4f-4700-be41-06912c5a04f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /workspace/AAIPL/hf_models/huggingface/models--Unsloth--Llama-3.1-8B-Instruct/snapshots/4699cc75b550f9c6f3173fb80f4703b62d946aa5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520b42d860fc4d86842045d463351854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "MODEL_ROOT = \"/workspace/AAIPL/hf_models/huggingface\"\n",
    "MODEL_FOLDER = \"models--Unsloth--Llama-3.1-8B-Instruct\"\n",
    "SNAPSHOTS = os.path.join(MODEL_ROOT, MODEL_FOLDER, \"snapshots\")\n",
    "\n",
    "snapshot_hash = os.listdir(SNAPSHOTS)[0]\n",
    "LOCAL_MODEL_PATH = os.path.join(SNAPSHOTS, snapshot_hash)\n",
    "\n",
    "print(\"Loading model from:\", LOCAL_MODEL_PATH)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./q_agent_sft_final\",\n",
    "    is_trainable=True  # 🔥 THIS IS CRUCIAL\n",
    ")\n",
    "\n",
    "model.train()\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "858ffa5a-1ac7-4416-8e78-2d201b5934e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ab38155ebf4dbc9993b51cb106efa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from agents.answer_model_1 import AAgent\n",
    "\n",
    "a_model = AAgent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c34a76-014a-409b-adaf-3586a77a02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def run_a_agent(question_json):\n",
    "    question = question_json[\"question\"]\n",
    "    choices = question_json[\"choices\"]\n",
    "\n",
    "    formatted_choices = \" \".join(choices)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Question: {question}\n",
    "Choices: {formatted_choices}\n",
    "\n",
    "Respond ONLY with JSON:\n",
    "{{\"answer\": \"A\"}}\n",
    "\"\"\"\n",
    "\n",
    "    response, _, _ = a_model.generate_response(\n",
    "        prompt,\n",
    "        system_prompt=\"You are a precise logical reasoning solver.\",\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response)\n",
    "        return parsed.get(\"answer\", None)\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fb0d5d7-1b80-4780-a434-27e75f2db12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "sample = generate_question_clean(\"Syllogisms\")\n",
    "question_json = json.loads(sample)\n",
    "print(run_a_agent(question_json))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3374a7ce-4889-41bc-b4fd-a9b5b719bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(topic):\n",
    "    prompt = build_prompt(topic)\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    chat = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(chat, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    full_sequence = output_ids[0]\n",
    "\n",
    "    return inputs[\"input_ids\"][0], full_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6727e02-3fb6-45ef-ae5b-1afdfe53e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(prompt_ids, full_sequence_ids):\n",
    "    with torch.enable_grad():\n",
    "\n",
    "        outputs = model(full_sequence_ids.unsqueeze(0))\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Shift for next-token prediction\n",
    "        shift_logits = logits[:, :-1, :]\n",
    "        shift_labels = full_sequence_ids[1:].unsqueeze(0)\n",
    "\n",
    "        log_probs = torch.log_softmax(shift_logits, dim=-1)\n",
    "\n",
    "        selected_log_probs = log_probs.gather(\n",
    "            2,\n",
    "            shift_labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        total_log_prob = selected_log_probs.sum()\n",
    "\n",
    "        return total_log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ef97f47-8dbe-4df2-9d59-e163fd303658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(q_json):\n",
    "    if not isinstance(q_json, dict):\n",
    "        return -3\n",
    "\n",
    "    if len(q_json.get(\"choices\", [])) != 4:\n",
    "        return -3\n",
    "\n",
    "    if \"answer\" not in q_json:\n",
    "        return -3\n",
    "\n",
    "    # Ask A-agent\n",
    "    a_pred = run_a_agent(q_json)\n",
    "\n",
    "    if a_pred is None:\n",
    "        return -2\n",
    "\n",
    "    if a_pred == q_json[\"answer\"]:\n",
    "        return 0.5      # easy question\n",
    "    else:\n",
    "        return 2.0      # fooled A-agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d227aeb-bbde-43ba-9be9-8b5c8e1b74b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Reward: 0.5\n",
      "Step 1 | Reward: 0.5\n",
      "Step 2 | Reward: 2.0\n",
      "Step 3 | Reward: 0.5\n",
      "Step 4 | Reward: 2.0\n",
      "Step 5 | Reward: 2.0\n",
      "Step 6 | Reward: 2.0\n",
      "Step 7 | Reward: 0.5\n",
      "Step 8 | Reward: 2.0\n",
      "Step 9 | Reward: 0.5\n",
      "Step 10 | Reward: 2.0\n",
      "Step 11 | Reward: 2.0\n",
      "Step 12 | Reward: 2.0\n",
      "Step 13 | Reward: 0.5\n",
      "Step 14 | Reward: 0.5\n",
      "Step 15 | Reward: 0.5\n",
      "Step 16 | Reward: 0.5\n",
      "Step 17 | Reward: 2.0\n",
      "Step 18 | Reward: 0.5\n",
      "Step 19 | Reward: 0.5\n",
      "Step 20 | Reward: 2.0\n",
      "Step 21 | Reward: 0.5\n",
      "Step 22 | Reward: 0.5\n",
      "Step 23 | Reward: 2.0\n",
      "Step 24 | Reward: 2.0\n",
      "Step 25 | Reward: 0.5\n",
      "Step 26 | Reward: 0.5\n",
      "Step 27 | Reward: 2.0\n",
      "Step 28 | Reward: 2.0\n",
      "Step 29 | Reward: 2.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "topics = [\n",
    "    \"Syllogisms\",\n",
    "    \"Seating Arrangements (Circular and Linear)\",\n",
    "    \"Blood Relations and Family Tree\",\n",
    "    \"Mixed Series (Alphanumeric)\"\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=5e-6\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "for step in range(30):\n",
    "\n",
    "    topic = random.choice(topics)\n",
    "\n",
    "    prompt_ids, full_sequence_ids = generate_sample(topic)\n",
    "\n",
    "    decoded = tokenizer.decode(\n",
    "        full_sequence_ids[prompt_ids.shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        q_json = json.loads(decoded)\n",
    "        reward = compute_reward(q_json)\n",
    "    except:\n",
    "        reward = -3\n",
    "\n",
    "    log_prob = compute_logprob(prompt_ids, full_sequence_ids)\n",
    "\n",
    "    loss = -reward * log_prob\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Step {step} | Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcb0c024-6076-4c38-b94a-b76e62fe33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./q_agent_rl_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "253a9a7e-aebc-4ac7-970f-d61ed4d1a00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8816dad6-28d2-4127-a684-7a1dd7c13094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_eval(topic):\n",
    "    prompt = build_prompt(topic)\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    chat = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(chat, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated = output_ids[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    decoded = tokenizer.decode(\n",
    "        generated,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e15a2d7-766f-4f82-8f2a-44aba1b7a45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1\n",
      "Topic: Syllogisms\n",
      "Correct: A\n",
      "A-Agent Pred: A\n",
      "\n",
      "Question 2\n",
      "Topic: Seating Arrangements (Circular and Linear)\n",
      "Correct: A\n",
      "A-Agent Pred: A\n",
      "\n",
      "Question 3\n",
      "Topic: Blood Relations and Family Tree\n",
      "Correct: C) Grandson\n",
      "A-Agent Pred: A\n",
      "\n",
      "Question 4\n",
      "Topic: Syllogisms\n",
      "Correct: A\n",
      "A-Agent Pred: A\n",
      "\n",
      "Question 5\n",
      "Topic: Syllogisms\n",
      "Correct: B\n",
      "A-Agent Pred: A\n",
      "\n",
      "Question 6\n",
      "Topic: Blood Relations and Family Tree\n",
      "Correct: B\n",
      "A-Agent Pred: A\n",
      "\n",
      "Question 7\n",
      "Topic: Mixed Series (Alphanumeric)\n",
      "Correct: C\n",
      "A-Agent Pred: A\n",
      "\n",
      "Question 8\n",
      "Topic: Blood Relations and Family Tree\n",
      "Correct: A\n",
      "A-Agent Pred: B\n",
      "\n",
      "Question 9\n",
      "Topic: Seating Arrangements (Circular and Linear)\n",
      "Correct: A\n",
      "A-Agent Pred: A\n",
      "\n",
      "Question 10\n",
      "Topic: Blood Relations and Family Tree\n",
      "Correct: A\n",
      "A-Agent Pred: A\n",
      "\n",
      "A-Agent Accuracy: 0.5\n",
      "A-Agent Failure Rate: 0.5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "fail_count = 0\n",
    "total = 10\n",
    "\n",
    "topics = [\n",
    "    \"Syllogisms\",\n",
    "    \"Seating Arrangements (Circular and Linear)\",\n",
    "    \"Blood Relations and Family Tree\",\n",
    "    \"Mixed Series (Alphanumeric)\"\n",
    "]\n",
    "\n",
    "for i in range(total):\n",
    "\n",
    "    topic = random.choice(topics)\n",
    "    decoded = generate_question_eval(topic)\n",
    "\n",
    "    try:\n",
    "        q_json = json.loads(decoded)\n",
    "    except:\n",
    "        print(\"Invalid JSON\")\n",
    "        continue\n",
    "\n",
    "    a_pred = run_a_agent(q_json)\n",
    "\n",
    "    correct = q_json[\"answer\"]\n",
    "\n",
    "    print(f\"\\nQuestion {i+1}\")\n",
    "    print(\"Topic:\", topic)\n",
    "    print(\"Correct:\", correct)\n",
    "    print(\"A-Agent Pred:\", a_pred)\n",
    "\n",
    "    if a_pred != correct:\n",
    "        fail_count += 1\n",
    "\n",
    "print(\"\\nA-Agent Accuracy:\", (total - fail_count) / total)\n",
    "print(\"A-Agent Failure Rate:\", fail_count / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "913b3a6a-c46f-45d5-b0da-b8b6e17f8b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Loading base model from: /workspace/AAIPL/hf_models/huggingface/models--Unsloth--Llama-3.1-8B-Instruct/snapshots/4699cc75b550f9c6f3173fb80f4703b62d946aa5\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:10<00:00,  2.56s/it]\n",
      "STEPS: 100%|██████████████████████████████████████| 4/4 [00:45<00:00, 11.43s/it]\n",
      "Generated 20 questions!\n",
      "{\n",
      "  \"topic\": \"Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"T5O, U6P, V7Q, W8R, _____\",\n",
      "  \"choices\": [\"A) X9S\", \"B) Y10T\", \"C) W9R\", \"D) X10S\"],\n",
      "  \"answer\": \"D\",\n",
      "  \"explanation\": \"The series involves two interleaved sequences: the first and third letters are alphabetical in order, and the second and fourth letters are in a numerical order. The first letter increases by one in the alphabet, and the second letter increases by one in the number sequence. Hence X10S.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree/Family tree logic\",\n",
      "  \"question\": \"Rohan says, “The person in the picture is the son of my mother’s husband’s mother.” How is the person in the picture related to Rohan?\",\n",
      "  \"choices\": [\"A) A) Grandfather\", \"B) B) Father\", \"C) C) Brother\", \"D) D) Uncle\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"Rohan's mother's husband is his father. The mother of his father is his paternal grandmother. Therefore, the person in the picture is Rohan's Brother.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"T4E, U5F, V6G, W7H, _____\",\n",
      "  \"choices\": [\"A) X8I\", \"B) X7I\", \"C) V9J\", \"D) W8K\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"The first letter of each term increases alphabetically, while the second letter is a digit that increases by 1, and the third letter increases alphabetically as well. The fourth letter is a letter that is one position ahead of the third letter in the alphabet. Hence X8I.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree/Family tree logic\",\n",
      "  \"question\": \"Rahul says, “The person in the picture is the wife of my husband’s mother’s brother.” How is the woman in the picture related to Rahul?\",\n",
      "  \"choices\": [\"A) A) Sister-in-law\", \"B) B) Aunt\", \"C) C) Mother-in-law\", \"D) D) Niece\"],\n",
      "  \"answer\": \"B\",\n",
      "  \"explanation\": \"Rahul's husband's mother's brother is his uncle. The wife of Rahul's uncle is his aunt.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree/Family tree logic\",\n",
      "  \"question\": \"Rahul says, 'The woman in the picture is the daughter of my father’s mother’s husband’s only daughter’s husband’s father.’ How is the woman related to Rahul?\",\n",
      "  \"choices\": [\"A) A) Aunt\", \"B) B) Grandmother\", \"C) C) Mother’s Sister\", \"D) D) Wife\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"Rahul's 'father's mother' refers to his grandmother. The husband of his grandmother is her spouse. The 'only daughter' of the spouse is Rahul's mother. The husband of Rahul's mother is his father. Therefore, the woman in the picture is the mother's sister, i.e., Rahul's Aunt.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree/Family tree logic\",\n",
      "  \"question\": \"Rahul says, “The person in the picture is the only son of my mother’s brother’s wife’s sister’s husband.” How is the person in the picture related to Rahul?\",\n",
      "  \"choices\": [\"A) A) Uncle\", \"B) B) Father\", \"C) C) Son\", \"D) D) Brother\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"Rahul's mother's brother is his uncle, making the brother's wife his aunt. The sister of the aunt is his great-aunt, and the husband of the great-aunt is the person in the picture. Since the person is the only son of Rahul's great-aunt, he is Rahul's uncle.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree/Family tree logic\",\n",
      "  \"question\": \"Kamal says, “The man in the picture is the brother of my mother’s mother’s husband’s son’s daughter’s husband’s mother.” How is the man related to Kamal?\",\n",
      "  \"choices\": [\"A) A) Grandfather\", \"B) B) Uncle\", \"C) C) Great Uncle\", \"D) D) Great Grandfather\"],\n",
      "  \"answer\": \"D\",\n",
      "  \"explanation\": \"Kamal's mother's mother is her grandmother. The husband of the grandmother is Kamal's great grandfather. The son of the great grandfather is Kamal's great grand uncle. The daughter of the great grand uncle is Kamal's great grand aunt. The husband of the great grand aunt is Kamal's great grandfather's brother, making the man in the picture Kamal's Great Grandfather.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"A1B2, C3D4, E5F6, G7H8, _____\",\n",
      "  \"choices\": [\"A) I9J10\", \"B) J10K11\", \"C) H9J10\", \"D) I8K9\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"The first letter progresses in alphabetical order: A, C, E, G, I. The second letter increases by 2 each time: 2, 4, 6, 8, 10. The third letter is the same as the first letter of the previous term, and the fourth letter is the same as the second letter of the previous term. Hence H9J10.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"A1B2, C3D4, E5F6, G7H8, _____\",\n",
      "  \"choices\": [\"A) I9J10\", \"B) H8J9\", \"C) G10H11\", \"D) I8J9\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"The first letter alternates between the first half of the alphabet (A, C, E, G) and the last half (B, D, F, H). The second letter increases by 2 each time. Therefore, the next term would follow the sequence of the last half of the alphabet, resulting in I9J10.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Seating Arrangements (Linear, Circular)\",\n",
      "  \"question\": \"In a circular arrangement of 8 people, A, B, C, D, E, F, G, and H are seated in a circle. A is seated 3 positions to the right of C. B is seated 2 positions to the left of E, who is seated opposite to F. G is seated 2 positions to the right of the person seated opposite to D. If the arrangement is rotated such that the person seated 2 positions to the left of E moves to the position of the person seated 2 positions to the right of G, and the person seated opposite to F takes the position of the person seated opposite to D, who will be seated 2 positions to the right of the person seated opposite to the new position of E?\",\n",
      "  \"choices\": [\"A) A) F\", \"B) B) G\", \"C) C) D\", \"D) D) H\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"After the rotation, E moves to G's right, and the person opposite F moves to D's position. The new arrangement is E-G-F-D-A-C-B-H. The person opposite E is G. Two positions to G's right is B.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Logical Reasoning/Syllogisms\",\n",
      "  \"question\": \"Statement I: All F is G\n",
      "Statement II: No G is H\n",
      "Statement III: Some H is J\n",
      "Conclusion I: Some J is not F\n",
      "Conclusion II: All F is J\",\n",
      "  \"choices\": [\"A) A) If only conclusion I follows\", \"B) B) If only conclusion II follows\", \"C) C) If conclusion I and II both follow\", \"D) D) If conclusion II alone is false\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"Since all F is G and no G is H, then no F is H. Therefore, some J being H means some J is not F.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"T5G, U6H, V7I, W8J, _____\",\n",
      "  \"choices\": [\"A) X9K, B) X9L, C) X9M, D) X9N\"],\n",
      "  \"answer\": \"B\",\n",
      "  \"explanation\": \"The pattern consists of two series: the first letter series and the second letter series. The first letter series increases alphabetically, and the second letter series increases numerically. The first letter of each term is the last letter of the previous term's first letter series, and the second letter is the previous term's second letter plus one. Therefore, the next term should be X9L.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"AB2C, CD3D, EF4E, GH5F, _____\",\n",
      "  \"choices\": [\"A) IJ6G\", \"B) JK6H\", \"C) IH6G\", \"D) IH7H\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"The first letter of each term alternates between A and C, then D and G. The second letter of each term is a number in ascending order. The third letter of each term alternates between B, D, E, and F. Therefore, the next term should follow the pattern of alternating first letters and the next number in the sequence, resulting in IH6G.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Seating Arrangements (Linear, Circular)\",\n",
      "  \"question\": \"Seven people, A, B, C, D, E, F, and G, are seated in a circular arrangement. A and B are seated together, with B to the left of A. C and D are seated together, with D to the left of C. E and F are seated together, with F to the left of E. G is seated between B and C. If the group is rearranged such that the people seated together are split apart and the people who were not seated together are paired, what is the position of the person originally seated between B and C?\",\n",
      "  \"choices\": [\"A) Between A and D\", \"B) Between A and E\", \"C) Between D and E\", \"D) Between F and G\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"When the people seated together are split apart, A and B move to opposite sides, as do C and D, and E and F. The only spot that remains between two people is between D and E.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Seating Arrangements (Linear, Circular)\",\n",
      "  \"question\": \"In a circular arrangement of 8 people, A, B, C, D, E, F, G, and H are seated around a circular table. A and C are seated opposite each other, and D and F are seated opposite each other. E is seated between B and G, who are seated opposite each other. If the people seated between A and C are moved to the seats opposite them, and the people seated between B and G are moved to the seats opposite them, who will be seated between A and C?\",\n",
      "  \"choices\": [\"A) B and G\", \"B) D and F\", \"C) E and H\", \"D) C and E\"],\n",
      "  \"answer\": \"B\",\n",
      "  \"explanation\": \"Initially, A and C are opposite each other, and D and F are opposite each other. E is between B and G, who are opposite each other. After the swap, the people between A and C (E and H) are moved to the seats opposite them, and the people between B and G (E and H) are moved to the seats opposite them. Since E and H are now in the seats opposite A and C, and D and F remain opposite each other, the correct answer is D and F.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Seating Arrangements (Linear, Circular)\",\n",
      "  \"question\": \"In a circular arrangement of 10 people, A, B, C, D, E, F, G, H, I, and J are seated around a table. A and J are seated opposite each other, with A to the left of J. F and E are seated opposite each other, with F to the right of E. C is seated two positions to the left of D. B is seated to the right of G. If the group rearranges themselves clockwise by two positions, and then the person seated opposite the person who was originally seated opposite A is now seated opposite the person who was originally seated opposite E, who will be seated opposite the person who was originally seated opposite the person now seated opposite E?\",\n",
      "  \"choices\": [\"A) B) F\", \"B) C) G\", \"C) D) I\", \"D) E) H\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"After the initial clockwise rotation, the person originally opposite A is now opposite E. The person originally opposite E is now opposite the person who was originally opposite A, who is now opposite F. So, the person opposite the person now opposite E is F's original opposite, which is E.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Seating Arrangements (Linear, Circular)\",\n",
      "  \"question\": \"In a circular arrangement of 10 people, A, B, C, D, E, F, G, H, I, and J are seated around a table. A and J are separated by 3 people, while C and F are 5 people apart. B is seated between A and D, and G is seated between E and H. If the seating arrangement is rotated clockwise by 3 positions, and then E and H switch places, who will be seated between the person originally seated to the immediate right of E and the person originally seated to the immediate left of H?\",\n",
      "  \"choices\": [\"A) B) D\", \"B) C) G\", \"C) D) F\", \"D) E) A\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"After rotating the arrangement clockwise by 3 positions, A, B, D, E, F, G, H, I, J, C becomes C, E, G, I, J, A, B, D, F, H. Then E and H switch places, resulting in C, H, G, I, J, A, B, D, F, E. The person to the right of E is G, and the person to the left of H is C. Therefore, G is seated between C and the person originally seated to the immediate left of H, which is now B.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree/Family tree logic\",\n",
      "  \"question\": \"Kavita says, “The woman in the picture is the wife of my father’s brother’s son.” How is the woman related to Kavita’s father?\",\n",
      "  \"choices\": [\"A) A) Daughter\", \"B) B) Mother\", \"C) C) Granddaughter\", \"D) D) Sister-in-law\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"Kavita’s father’s brother is her uncle. The son of her uncle is her cousin. The wife of her cousin is her cousin’s wife. Therefore, the woman in the picture is the wife of Kavita’s cousin, making her Kavita’s father’s daughter, i.e., his daughter.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"T4U, S3R, R2Q, P1O, _____\",\n",
      "  \"choices\": [\"A) O0N\", \"B) N0M\", \"C) M0L\", \"D) Q2P\"],\n",
      "  \"answer\": \"D\",\n",
      "  \"explanation\": \"The pattern involves two separate series. The first series consists of decreasing letters (T, S, R, P), and the second series consists of decreasing numbers (4, 3, 2, 1). The correct next term should continue this pattern, resulting in Q2P.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Logical Reasoning/Syllogisms\",\n",
      "  \"question\": \"Statement I: All mammals are animals\n",
      "Statement II: All animals are living beings\n",
      "Statement III: Some living beings are plants\n",
      "Conclusion I: Some mammals are plants\n",
      "Conclusion II: All mammals are living beings\",\n",
      "  \"choices\": [\n",
      "    \"A) A) If only conclusion I follows\",\n",
      "    \"B) B) If only conclusion II follows\",\n",
      "    \"C) C) If conclusion I and II both follow\",\n",
      "    \"D) D) If conclusion II is a logical consequence of the given statements\"\n",
      "  ],\n",
      "  \"answer\": \"D\",\n",
      "  \"explanation\": \"Conclusion II is a direct logical consequence of Statement I and Statement II, as all mammals are indeed living beings by virtue of being animals.\"\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Time taken per batch generation: [19.757675170898438, 8.145927906036377, 8.432912111282349, 9.342996597290039]\n",
      "Tokens generated per batch: [870, 1365, 1405, 1555]\n",
      "Total Time Taken: 45.680 seconds; Total Tokens: 5195; TGPS: 113.727 seconds\n",
      "\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Invalid JSON format in question: {\n",
      "  \"topic\": \"Logical Reasoning/Syllogisms\",\n",
      "  \"question\": \"Statement I: All F is G\n",
      "Statement II: No G is H\n",
      "Statement III: Some H is J\n",
      "Conclusion I: Some J is not F\n",
      "Conclusion II: All F is J\",\n",
      "  \"choices\": [\"A) A) If only conclusion I follows\", \"B) B) If only conclusion II follows\", \"C) C) If conclusion I and II both follow\", \"D) D) If conclusion II alone is false\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"Since all F is G and no G is H, then no F is H. Therefore, some J being H means some J is not F.\"\n",
      "}\n",
      "Error: Invalid control character at: line 3 column 39 (char 83)\n",
      "Invalid JSON format in question: {\n",
      "  \"topic\": \"Logical Reasoning/Syllogisms\",\n",
      "  \"question\": \"Statement I: All mammals are animals\n",
      "Statement II: All animals are living beings\n",
      "Statement III: Some living beings are plants\n",
      "Conclusion I: Some mammals are plants\n",
      "Conclusion II: All mammals are living beings\",\n",
      "  \"choices\": [\n",
      "    \"A) A) If only conclusion I follows\",\n",
      "    \"B) B) If only conclusion II follows\",\n",
      "    \"C) C) If conclusion I and II both follow\",\n",
      "    \"D) D) If conclusion II is a logical consequence of the given statements\"\n",
      "  ],\n",
      "  \"answer\": \"D\",\n",
      "  \"explanation\": \"Conclusion II is a direct logical consequence of Statement I and Statement II, as all mammals are indeed living beings by virtue of being animals.\"\n",
      "}\n",
      "Error: Invalid control character at: line 3 column 52 (char 96)\n",
      "Saved to outputs/questions.json!\n"
     ]
    }
   ],
   "source": [
    "!python -m agents.question_agent \\\n",
    "    --num_questions 20 \\\n",
    "    --output_file outputs/questions.json \\\n",
    "    --batch_size 5 \\\n",
    "    --verbose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df32c9f2-575b-4c12-8d7d-ca377e254150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /workspace/AAIPL/hf_models/huggingface/models--Qwen--Qwen2.5-14B-Instruct/snapshots/cf98f3b3bbb457ad9e2bb7baf9a0125b6b88caa8\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "MODEL_ROOT = \"/workspace/AAIPL/hf_models/huggingface\"\n",
    "\n",
    "MODEL_FOLDER = \"models--Qwen--Qwen2.5-14B-Instruct\"\n",
    "SNAPSHOTS = os.path.join(MODEL_ROOT, MODEL_FOLDER, \"snapshots\")\n",
    "\n",
    "# Get actual snapshot folder\n",
    "snapshot_hash = os.listdir(SNAPSHOTS)[0]\n",
    "LOCAL_MODEL_PATH = os.path.join(SNAPSHOTS, snapshot_hash)\n",
    "\n",
    "print(\"Loading model from:\", LOCAL_MODEL_PATH)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    local_files_only=True\n",
    ")\n",
    "def count_tokens_q(text: str) -> int:\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def filter_questions(questions: List[str|Dict[str, str|Any]]) -> List[Dict[str, str|Any]]:\n",
    "    def basic_checks(q2: Dict[str, str])->bool:\n",
    "        # check required keys\n",
    "        required_keys = ['topic', 'question', 'choices', 'answer']\n",
    "        if all((key in q2) for key in required_keys):\n",
    "            # check choices format\n",
    "            checks = all(isinstance(choice, str) and len(choice) > 2 and choice[0].upper() in 'ABCD' for choice in q2['choices'])\n",
    "            if isinstance(q2['choices'], list) and len(q2['choices']) == 4 and checks:\n",
    "                # check answer format\n",
    "                # Check token length\n",
    "                check_len = sum(count_tokens_q(q2[k]) for k in ['question', 'answer'])\n",
    "                check_len += sum(count_tokens_q(choice) for choice in q2['choices']) - 15\n",
    "                if check_len < 130:\n",
    "                    if check_len + count_tokens_q(q2.get('explanation', 'None')) <= 1024:\n",
    "                        # Extra Checks: (PLUS checks) len(q2['answer']) == 1 and q2['answer'].upper() in 'ABCD':\n",
    "                        if isinstance(q2['answer'], str):\n",
    "                            return True\n",
    "        return False\n",
    "    correct_format_question = []\n",
    "    for i, q in enumerate(questions):\n",
    "        if isinstance(q, dict):\n",
    "            if basic_checks(q):\n",
    "                correct_format_question.append(q)\n",
    "        elif isinstance(q, str):\n",
    "            try:\n",
    "                q1 = json.loads(q)\n",
    "                if basic_checks(q1):\n",
    "                    correct_format_question.append(q1)\n",
    "            except json.JSONDecodeError:\n",
    "                # If JSON decoding fails, skip this answer\n",
    "                print(f\"Skipping invalid JSON at index {i}: {q}\")\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "    if len(correct_format_question) >= 0.5 * len(questions):\n",
    "        return correct_format_question\n",
    "    return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3b0968c-3996-4ea1-a136-d270e15c3696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 20\n",
      "Number of filtered questions: 11\n"
     ]
    }
   ],
   "source": [
    "with open(\"outputs/questions.json\", \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "filtered_questions = filter_questions(questions)\n",
    "\n",
    "with open(\"outputs/filtered_questions.json\", \"w\") as f:\n",
    "    json.dump(filtered_questions, f, indent=4)\n",
    "\n",
    "print(f\"Number of questions: {len(questions)}\")\n",
    "print(f\"Number of filtered questions: {len(filtered_questions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7e333ea-8d52-41ae-a2f9-831509055ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Loading base model from: /workspace/AAIPL/hf_models/huggingface/models--Unsloth--Llama-3.1-8B-Instruct/snapshots/4699cc75b550f9c6f3173fb80f4703b62d946aa5\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:09<00:00,  2.37s/it]\n",
      "STEPS: 100%|██████████████████████████████████████| 4/4 [00:42<00:00, 10.61s/it]\n",
      "Generated 20 questions!\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree/Family tree logic\",\n",
      "  \"question\": \"Rahul says, “My mother’s husband is the son of my father’s mother’s husband.” How is Rahul’s mother related to the man in the picture?\",\n",
      "  \"choices\": [\"A) A) Wife\", \"B) B) Daughter\", \"C) C) Granddaughter\", \"D) D) Sister-in-law\"],\n",
      "  \"answer\": \"B\",\n",
      "  \"explanation\": \"Rahul's'mother's husband' is his father. His 'father's mother' is his paternal grandmother. The 'husband' of his paternal grandmother is his grandfather. Therefore, Rahul's mother is the daughter of his grandfather, making her the man in the picture.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree/Family tree logic\",\n",
      "  \"question\": \"Rahul says, “The woman in the picture is the wife of my father’s brother’s son.” How is the woman related to Rahul?\",\n",
      "  \"choices\": [\"A) A) Mother\", \"B) B) Sister\", \"C) C) Aunt\", \"D) D) Niece\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"Rahul's 'father's brother' refers to his paternal uncle. The son of his paternal uncle is his first cousin. Therefore, the woman in the picture is the wife of his first cousin, making her his Aunt.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Seating Arrangements (Linear, Circular)\",\n",
      "  \"question\": \"In a circular arrangement of 8 people, A, B, C, D, E, F, G, and H, seated at a round table with A directly across from G. If B and E switch seats, C and F switch seats, and D moves one position to the right, where will D be seated in relation to A after the rearrangement?\",\n",
      "  \"choices\": [\"A) D will be two positions to the right of A\", \"B) D will be one position to the left of A\", \"C) D will be one position to the right of A\", \"D) D will be three positions to the left of A\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"Initially, A and G are directly across from each other. When D moves one position to the right, it will shift from its original position relative to A, but the direct opposite relationship remains. After the switch, the only position that maintains the direct opposite relationship is two positions to the right of A.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Logical Reasoning/Syllogisms\",\n",
      "  \"question\": \"Statement I: All A are B\n",
      "Statement II: No B is C\n",
      "Statement III: Some C are D\n",
      "Conclusion I: All A are D\n",
      "Conclusion II: Some D are A\",\n",
      "  \"choices\": [\n",
      "    \"A) A) If only conclusion I follows\",\n",
      "    \"B) B) If only conclusion II follows\",\n",
      "    \"C) C) If conclusion I and II both follow\",\n",
      "    \"D) D) If conclusion II and III are contradictory\"\n",
      "  ],\n",
      "  \"answer\": \"B\",\n",
      "  \"explanation\": \"Statement II establishes a non-overlapping relationship between B and C, ensuring that no A can be D. Conclusion I is impossible, but conclusion II is possible because some C can still be D, even though no B is C.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Seating Arrangements (Linear, Circular)\",\n",
      "  \"question\": \"Five friends, A, B, C, D, and E, are seated in a circular arrangement. A is seated three positions to the right of B. D is seated exactly opposite to E. C is seated two positions to the left of the person who is seated opposite to A. If B and E switch seats, and the person seated opposite to C moves to the seat of the person seated opposite to A, who will be seated exactly opposite to C?\",\n",
      "  \"choices\": [\"A) A) B\", \"B) B) E\", \"C) C) D\", \"D) D) A\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"Initially, A is three positions to the right of B. After the swap, E becomes the new B. C is two positions to the left of the person opposite A. Since C is opposite to A, it means C is two positions to the right of E. The person opposite to C will be the one two positions to the left of E, which is B.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"G2H, E4I, C6J, A8K, _____\",\n",
      "  \"choices\": [\"A) B0L\", \"B) F2M\", \"C) A10L\", \"D) D6M\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"The pattern involves two series: alphabetical order of the first letter in reverse (G, E, C, A) and an increasing number series (2, 4, 6, 8). The second letter 'H' appears in each term, and the number series increases by 2. Therefore, the next term should have the first letter 'A' from the alphabetical order and the next number in the series, which is 10, and the second letter remains 'L'.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"3^1, 4^2, 5^3, 6^4, _____\",\n",
      "  \"choices\": [\"A) 7^5\", \"B) 2^6\", \"C) 1^7\", \"D) 7^5\"],\n",
      "  \"answer\": \"D\",\n",
      "  \"explanation\": \"The pattern involves two series: the base series (3, 4, 5, 6,...) and the exponent series (1, 2, 3, 4,...). The correct term should follow the base series incrementing by 1 and the exponent series incrementing by 1. Hence, the next base is 7 and the next exponent is 5, making 7^5.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Seating Arrangements (Linear, Circular)\",\n",
      "  \"question\": \"In a circular arrangement of 8 people, A, B, C, D, E, F, G, and H are seated in a circle. A and C are seated 3 positions apart. D and F are seated 2 positions apart. E is seated 5 positions apart from G. If the arrangement is rotated clockwise by 2 positions, and then E swaps places with the person seated 4 positions to their right, who will be seated 3 positions to the left of the person originally seated 5 positions to the right of E?\",\n",
      "  \"choices\": [\"A) A) D\", \"B) B) C\", \"C) C) F\", \"D) D) H\"],\n",
      "  \"answer\": \"B\",\n",
      "  \"explanation\": \"After rotation, the arrangement changes. E's swap with the person 4 positions to their right doesn't change the relative positions of A and C, but it does change E's position. The person originally 5 positions to the right of E is now 5 positions to the right of the person E swapped with. Since E and this person were originally 5 positions apart, they remain 5 positions apart. A is 3 positions to the left of the person 5 positions to the right of E.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"3^2, 5^2, 7^2, 11^2, 15^2, _____\",\n",
      "  \"choices\": [\"A) 21^2\", \"B) 19^2\", \"C) 23^2\", \"D) 17^2\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"The pattern alternates between consecutive prime numbers. The first term is 3^2, the second term is 5^2, the third term is 7^2, the fourth term is 11^2, and the fifth term would be 15^2 is incorrect, but 15 is not a prime number. The next prime number after 11 is 13, but the pattern alternates between consecutive prime numbers, so the next prime number after 15 is 17, but the correct sequence is 3, 5, 7, 11, 13, 15, 17, 19, 21, 23. Hence 21^2.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"A1B2C3, B2C3D4, C3D4E5, D4E5F6, _____\",\n",
      "  \"choices\": [\"A) E5F6G7\", \"B) F6G7H8\", \"C) E6F7G8\", \"D) E5G7H9\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"The pattern consists of two series: the first series is an alphabetical series of letters (A, B, C, D, E), and the second series is a numerical series (1, 2, 3, 4, 5). The numbers increase by 1, and the letters shift one position forward in the alphabet. Hence, the next term would be E6F7G8.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"The series consists of letters and numbers: 3A1, 4B2, 5C3, 6D4, _____\",\n",
      "  \"choices\": [\"A) 7E5\", \"B) 7D5\", \"C) 6E6\", \"D) 8E6\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"The series follows a pattern where the first digit increases by 1, the letter progresses alphabetically, and the second digit increases by 1. Hence, the next term should have 7 as the first digit, E as the second letter, and 5 as the third digit.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"G_3E, H_4F, I_5G, J_6H, _____\",\n",
      "  \"choices\": [\"A) K_7I\", \"B) K_8J\", \"C) J_7I\", \"D) L_9K\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"The first letter progresses alphabetically, and the second letter increases in value, then the third letter repeats the second letter of each term. Hence K_7I.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Syllogisms\",\n",
      "  \"question\": \"Statement I: All unicorns are dragons\n",
      "Statement II: No dragon is a phoenix\n",
      "Statement III: All phoenixes are mythical creatures\n",
      "Conclusion I: Some mythical creatures are unicorns\n",
      "Conclusion II: Some unicorns are phoenixes\",\n",
      "  \"choices\": [\"A) A) If only conclusion I follows\", \"B) B) If only conclusion II follows\", \"C) C) If conclusion I and II both follow\", \"D) D) If neither conclusion I nor conclusion II follows\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"Conclusion I is true because unicorns are mythical creatures. Conclusion II is false because no unicorn is a phoenix.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Seating Arrangements (Linear, Circular)\",\n",
      "  \"question\": \"In a circular arrangement of 8 people, A, B, C, D, E, F, G, and H, where A and B are diagonally opposite each other, and the other pairs are also diagonally opposite, and the clockwise order of the first four people is A-B-C-D. If the order of the last four people is reversed, and the first four people remain in the same order, who will be seated two positions to the left of the person seated directly across from B?\",\n",
      "  \"choices\": [\"A) C\", \"B) D\", \"C) E\", \"D) F\"],\n",
      "  \"answer\": \"A\",\n",
      "  \"explanation\": \"Reversing the last four people results in the order A-B-C-D-F-G-H-E. The person across from B is F. Two positions to the left of F is C.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Syllogisms\",\n",
      "  \"question\": \"Statement I: All mammals are warm-blooded\n",
      "Statement II: All birds are warm-blooded\n",
      "Statement III: All warm-blooded animals are homeothermic\n",
      "Conclusion I: All birds are homeothermic\n",
      "Conclusion II: All mammals are homeothermic\",\n",
      "  \"choices\": [\n",
      "    \"A) A) If only conclusion I follow\",\n",
      "    \"B) B) If only conclusion II follows\",\n",
      "    \"C) C) If conclusion I and II both follow\",\n",
      "    \"D) D) If conclusion II follows, but conclusion I does not necessarily follow\"\n",
      "  ],\n",
      "  \"answer\": \"D\",\n",
      "  \"explanation\": \"Since all birds are warm-blooded, conclusion I follows. However, not all mammals are birds, so conclusion II does not necessarily follow from the given statements.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree\",\n",
      "  \"question\": \"Rahul says, ‘The woman in the picture is the daughter of my mother’s husband’s mother’s husband.’ How is the woman related to Rahul?\",\n",
      "  \"choices\": [\"A) A) Mother\", \"B) B) Grandmother\", \"C) C) Great Aunt\", \"D) D) Great Grandmother\"],\n",
      "  \"answer\": \"D\",\n",
      "  \"explanation\": \"Rahul's mother's husband's mother is his grandmother. The husband of his grandmother is his great-grandfather. The daughter of his great-grandfather is his great-grandmother. Therefore, the woman in the picture is Rahul's great-grandmother.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree/Family tree logic\",\n",
      "  \"question\": \"Rahul says, “My mother’s brother’s daughter is the wife of my brother’s son’s father.” How is Rahul related to the woman?\",\n",
      "  \"choices\": [\"A) A) Grandmother\", \"B) B) Mother\", \"C) C) Aunt\", \"D) D) Mother-in-law\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"Rahul's mother's brother is his paternal uncle. The daughter of his paternal uncle is his paternal aunt's daughter, making her his cousin. The wife of his brother's son's father is his brother's wife, which is his sister-in-law. However, since his sister-in-law is his brother's wife, his brother's son's father is his brother, and his brother's son is his nephew. Therefore, his brother's son's father is his uncle, making his brother's son's father's wife his aunt.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"ABCD, EFGH, IJKL, MNOP, _____\",\n",
      "  \"choices\": [\"A) QRSTU\", \"B) QRSTV\", \"C) QRSUV\", \"D) QRTSU\"],\n",
      "  \"answer\": \"B\",\n",
      "  \"explanation\": \"The series alternates between two sequences: the first sequence consists of consecutive letters in the alphabet, and the second sequence consists of consecutive letters in reverse order of the alphabet. The pattern continues with Q, R, S, T, V.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Blood Relations and Family Tree/Family tree logic\",\n",
      "  \"question\": \"Rohan says, “The woman in the picture is the wife of my son’s mother’s only brother.” How is the woman related to Rohan?\",\n",
      "  \"choices\": [\"A) A) Daughter\", \"B) B) Granddaughter\", \"C) C) Sister\", \"D) D) Niece\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"Rohan's'son's mother' refers to Rohan's daughter. The 'only brother' of Rohan's daughter is Rohan's son. The wife of Rohan's son's brother is Rohan's sister, making the woman in the picture his Sister.\"\n",
      "}\n",
      "{\n",
      "  \"topic\": \"Series and Patterns/Mixed Series (Alphanumeric)\",\n",
      "  \"question\": \"T_9F, U_8E, V_7D, W_6C, _____\",\n",
      "  \"choices\": [\"A) X_5B\", \"B) X_5B\", \"C) Y_4A\", \"D) Z_3Z\"],\n",
      "  \"answer\": \"B\",\n",
      "  \"explanation\": \"The first letter increases alphabetically, and the second letter decreases numerically. The third letter follows the alphabetical order of the first letter of the previous term, and the fourth letter decreases numerically. Hence, the correct sequence is maintained in option B.\"\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Time taken per batch generation: [21.570258617401123, 8.670377016067505, 6.106157302856445, 6.056366920471191]\n",
      "Tokens generated per batch: [1180, 1415, 1000, 1050]\n",
      "Total Time Taken: 42.403 seconds; Total Tokens: 4645; TGPS: 109.544 seconds\n",
      "\n",
      "\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Invalid JSON format in question: {\n",
      "  \"topic\": \"Logical Reasoning/Syllogisms\",\n",
      "  \"question\": \"Statement I: All A are B\n",
      "Statement II: No B is C\n",
      "Statement III: Some C are D\n",
      "Conclusion I: All A are D\n",
      "Conclusion II: Some D are A\",\n",
      "  \"choices\": [\n",
      "    \"A) A) If only conclusion I follows\",\n",
      "    \"B) B) If only conclusion II follows\",\n",
      "    \"C) C) If conclusion I and II both follow\",\n",
      "    \"D) D) If conclusion II and III are contradictory\"\n",
      "  ],\n",
      "  \"answer\": \"B\",\n",
      "  \"explanation\": \"Statement II establishes a non-overlapping relationship between B and C, ensuring that no A can be D. Conclusion I is impossible, but conclusion II is possible because some C can still be D, even though no B is C.\"\n",
      "}\n",
      "Error: Invalid control character at: line 3 column 40 (char 84)\n",
      "Invalid JSON format in question: {\n",
      "  \"topic\": \"Syllogisms\",\n",
      "  \"question\": \"Statement I: All unicorns are dragons\n",
      "Statement II: No dragon is a phoenix\n",
      "Statement III: All phoenixes are mythical creatures\n",
      "Conclusion I: Some mythical creatures are unicorns\n",
      "Conclusion II: Some unicorns are phoenixes\",\n",
      "  \"choices\": [\"A) A) If only conclusion I follows\", \"B) B) If only conclusion II follows\", \"C) C) If conclusion I and II both follow\", \"D) D) If neither conclusion I nor conclusion II follows\"],\n",
      "  \"answer\": \"C\",\n",
      "  \"explanation\": \"Conclusion I is true because unicorns are mythical creatures. Conclusion II is false because no unicorn is a phoenix.\"\n",
      "}\n",
      "Error: Invalid control character at: line 3 column 53 (char 79)\n",
      "Invalid JSON format in question: {\n",
      "  \"topic\": \"Syllogisms\",\n",
      "  \"question\": \"Statement I: All mammals are warm-blooded\n",
      "Statement II: All birds are warm-blooded\n",
      "Statement III: All warm-blooded animals are homeothermic\n",
      "Conclusion I: All birds are homeothermic\n",
      "Conclusion II: All mammals are homeothermic\",\n",
      "  \"choices\": [\n",
      "    \"A) A) If only conclusion I follow\",\n",
      "    \"B) B) If only conclusion II follows\",\n",
      "    \"C) C) If conclusion I and II both follow\",\n",
      "    \"D) D) If conclusion II follows, but conclusion I does not necessarily follow\"\n",
      "  ],\n",
      "  \"answer\": \"D\",\n",
      "  \"explanation\": \"Since all birds are warm-blooded, conclusion I follows. However, not all mammals are birds, so conclusion II does not necessarily follow from the given statements.\"\n",
      "}\n",
      "Error: Invalid control character at: line 3 column 57 (char 83)\n",
      "Saved to outputs/filtered_questions.json!\n"
     ]
    }
   ],
   "source": [
    "!python -m agents.question_agent \\\n",
    "    --num_questions 20 \\\n",
    "    --output_file outputs/filtered_questions.json \\\n",
    "    --batch_size 5 \\\n",
    "    --verbose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c3b5f-92d9-4ae2-9fef-56ce79ac7cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
